{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ef115af",
   "metadata": {},
   "source": [
    "# PGD attacks\n",
    "\n",
    "This notebook tests **EuroSAT ResNet18 model** on images modified with **PGD attacks**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "859d9f70",
   "metadata": {},
   "source": [
    "### 1. Setup environment and imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d449ca6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os, re\n",
    "sys.path.append(os.path.abspath(\"..\"))\n",
    "\n",
    "if not hasattr(sys, \"frozen\"):\n",
    "    os.environ[\"PYDEVD_DISABLE_FILE_VALIDATION\"] = \"1\"\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import glob\n",
    "import numpy as np\n",
    "import tifffile\n",
    "from torchvision import transforms\n",
    "\n",
    "from src.training.simple_cnn import SimpleCNN\n",
    "from src.data.dataloader import get_dataloaders\n",
    "from src.attacks.evaluate import evaluate_pgd             \n",
    "from src.attacks.metrics_eval import evaluate_adv, plot_confusion_matrix  \n",
    "from src.attacks.utils import unnormalize,select_rgb_bands, gdal_style_scale, scale_for_display, load_and_prepare, DEFAULT_MEAN,DEFAULT_STD\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "model_name = \"resnet50\" # or resnet18 simplecnn\n",
    "data_dir = '../data/raw'\n",
    "out_dir = '../data/adv/adv_4'\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e182d54",
   "metadata": {},
   "source": [
    "- Get path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4903627",
   "metadata": {},
   "outputs": [],
   "source": [
    "if model_name == \"resnet18\":\n",
    "        checkpoint_path = \"../experiments/checkpoints/resnet18_e.pth\"\n",
    "elif model_name == \"resnet50\":\n",
    "        checkpoint_path = \"../experiments/checkpoints/resnet50_e2.pth\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "528f49e1",
   "metadata": {},
   "source": [
    "### 2. Load data and model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "946b611e",
   "metadata": {},
   "source": [
    "- Load dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d388079",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, val_loader, test_loader, classes = get_dataloaders(data_dir=data_dir, batch_size=batch_size)\n",
    "print(f'Loaded {len(classes)} classes: {classes}')\n",
    "\n",
    "num_classes = len(classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ac97b7d",
   "metadata": {},
   "source": [
    "- Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b269473",
   "metadata": {},
   "outputs": [],
   "source": [
    "if model_name.lower() == \"resnet18\":\n",
    "    model = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)\n",
    "    model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
    "elif model_name.lower() == \"resnet50\":\n",
    "    model = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V1)\n",
    "    model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
    "elif model_name.lower() == \"simplecnn\":\n",
    "    model = SimpleCNN(num_classes=num_classes)\n",
    "else:\n",
    "    raise ValueError(f\"Unsupported model_name: {model_name}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c2e04b4",
   "metadata": {},
   "source": [
    "- Load trained weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c6e8601",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(checkpoint_path, map_location=device))\n",
    "model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a616a35",
   "metadata": {},
   "source": [
    "### 3. Run PGD attacks and save adversarial images "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d71445b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "eps = 0.0005\n",
    "alpha = eps / 4\n",
    "iters = 20\n",
    "smooth_sigma=1.0\n",
    "\n",
    "res = evaluate_pgd(\n",
    "    model=model,\n",
    "    dataloader=test_loader,\n",
    "    device=device,\n",
    "    eps=eps,           \n",
    "    alpha=alpha,        \n",
    "    iters=iters,\n",
    "    out_dir=out_dir,\n",
    "    save_every=1,\n",
    "    max_save=100,\n",
    "    smooth_sigma=smooth_sigma,                     \n",
    "\n",
    ")\n",
    "\n",
    "print('PGD run result:', res)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c941db19",
   "metadata": {},
   "source": [
    "### 4. Evaluate saved adversarial images for each epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b27ba63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificar tamaño de entrada del modelo\n",
    "print(\"\\n=== Veryfing entry image size accepted by the model ===\")\n",
    "try:\n",
    "    dummy_input = torch.randn(1, 3, 224, 224).to(device)\n",
    "    output = model(dummy_input)\n",
    "    print(f\"✓ The models accepts 224x224\")\n",
    "except:\n",
    "    print(f\"✗ The models DO NOT accepts 224x224, trying 64x64...\")\n",
    "    try:\n",
    "        dummy_input = torch.randn(1, 3, 64, 64).to(device)\n",
    "        output = model(dummy_input)\n",
    "        print(f\"✓ The models accepts 64x64\")\n",
    "    except:\n",
    "        print(f\"✗ Error\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fc6ad9f",
   "metadata": {},
   "source": [
    "- Evaluation of metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d59f2d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\n=== Model Evaluation on Adversarial Images ===\")\n",
    "\n",
    "metrics_adv = evaluate_adv(\n",
    "    adv_folder=out_dir,\n",
    "    model_path=checkpoint_path,\n",
    "    data_dir=data_dir,\n",
    "    batch_size=batch_size,\n",
    "    model_name=model_name,\n",
    "    device=device,\n",
    "    mean_std_sample_size=2000,\n",
    "    image_pattern=\"*.tif\"\n",
    ")\n",
    "\n",
    "print(f\"Num images: {metrics_adv['num_images']}\")\n",
    "\n",
    "print(f\"Accuracy: {metrics_adv['accuracy']*100:.2f}%\")\n",
    "print(f\"Loss: {metrics_adv['loss']:.4f}\")\n",
    "print(f\"Precision: {metrics_adv['precision']:.4f}\")\n",
    "print(f\"Recall: {metrics_adv['recall']:.4f}\")\n",
    "print(f\"F1-score: {metrics_adv['f1']:.4f}\")\n",
    "\n",
    "print(\"\\nClassification metrics per category:\\n\\n\", metrics_adv[\"classification_report\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aa240e1",
   "metadata": {},
   "source": [
    "- Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01c2d153",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(metrics_adv['confusion_matrix'], metrics_adv['class_names'], normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f77f19d1",
   "metadata": {},
   "source": [
    "- Show a small sample of images "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eca783b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show a small sample of images \n",
    "adv_folder = out_dir\n",
    "raw_folder = '../data/raw'\n",
    "\n",
    "adv_paths = sorted(glob.glob(os.path.join(adv_folder, \"*.tif\")))\n",
    "\n",
    "# --- Match original raw images ---\n",
    "pairs = []\n",
    "for adv_p in adv_paths:\n",
    "    base = os.path.basename(adv_p)\n",
    "    orig_base = re.sub(r\"_true\\d+_pred\\d+\\.tif$\", \".tif\", base)\n",
    "    orig_candidates = glob.glob(os.path.join(raw_folder, \"**\", orig_base), recursive=True)\n",
    "    if not orig_candidates:\n",
    "        continue\n",
    "    pairs.append((adv_p, orig_candidates[0]))\n",
    "\n",
    "if len(pairs) == 0:\n",
    "    print(\"No matching pairs found.\")\n",
    "else:\n",
    "    random.shuffle(pairs)\n",
    "    pairs = pairs[:10]\n",
    "\n",
    "    fig, axs = plt.subplots(len(pairs), 3, figsize=(12, 4 * len(pairs)), dpi=200)\n",
    "    axs = np.atleast_2d(axs)\n",
    "    \n",
    "    # Extract mean and std from dataloader\n",
    "    mean, std = DEFAULT_MEAN, DEFAULT_STD\n",
    "    try:\n",
    "        ds = test_loader.dataset\n",
    "        if hasattr(ds, \"dataset\"):\n",
    "            base_ds = ds.dataset\n",
    "        else:\n",
    "            base_ds = ds\n",
    "        transform_obj = getattr(base_ds, \"transform\", None)\n",
    "        if transform_obj and hasattr(transform_obj, \"transforms\"):\n",
    "            for t in transform_obj.transforms:\n",
    "                if t.__class__.__name__ == \"Normalize\":\n",
    "                    mean = t.mean\n",
    "                    std = t.std\n",
    "                    break\n",
    "    except Exception:\n",
    "        pass\n",
    "    \n",
    "    MODEL_INPUT_SIZE = (224, 224)\n",
    "    \n",
    "    pred_transform = transforms.Compose([\n",
    "        transforms.Resize(MODEL_INPUT_SIZE),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=mean, std=std)\n",
    "    ])\n",
    "    \n",
    "    display_transform = transforms.Compose([\n",
    "        transforms.Resize(MODEL_INPUT_SIZE),\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    for i, (adv_p, orig_p) in enumerate(pairs): \n",
    "        # Cargar imagen original para visualización\n",
    "        orig_img = load_and_prepare(orig_p, size=MODEL_INPUT_SIZE, is_raw=True)\n",
    "        \n",
    "        # *** FIX: Las imágenes adversariales están guardadas como uint8 RGB [0-255] ***\n",
    "        adv_raw = tifffile.imread(adv_p)\n",
    "        \n",
    "        # Convertir a HWC si es necesario\n",
    "        if adv_raw.ndim == 3 and adv_raw.shape[0] in [3,4,13]:\n",
    "            adv_raw = np.transpose(adv_raw, (1,2,0))\n",
    "        \n",
    "        # Las adversariales ya son RGB uint8, solo tomar primeras 3 bandas\n",
    "        if adv_raw.shape[2] >= 3:\n",
    "            adv_rgb = adv_raw[:, :, :3]\n",
    "        else:\n",
    "            adv_rgb = adv_raw\n",
    "        \n",
    "        # Resize\n",
    "        from skimage.transform import resize\n",
    "        adv_rgb_resized = resize(adv_rgb, MODEL_INPUT_SIZE, preserve_range=True, anti_aliasing=True)\n",
    "        \n",
    "        # CRÍTICO: Las adversariales son uint8 [0-255], simplemente normalizar a [0,1]\n",
    "        # NO aplicar gdal_style_scale porque no son valores raw\n",
    "        adv_img_display = np.clip(adv_rgb_resized, 0, 255).astype(np.float32) / 255.0\n",
    "        \n",
    "        # Convertir a PIL para predicción (usar el mismo adv_rgb ya procesado)\n",
    "        adv_pil = Image.fromarray(adv_rgb.astype(np.uint8))\n",
    "        \n",
    "        # Para PREDICCIÓN: con normalización\n",
    "        adv_tensor = pred_transform(adv_pil).unsqueeze(0).to(device)\n",
    "        \n",
    "        # Get true class name from original path\n",
    "        true_class_name = os.path.basename(os.path.dirname(orig_p))\n",
    "        \n",
    "        # Predict adversarial image\n",
    "        try:\n",
    "            with torch.no_grad():\n",
    "                outputs = model(adv_tensor)\n",
    "                pred_idx = outputs.argmax(1).item()\n",
    "                pred_class_name = classes[pred_idx]\n",
    "        except Exception as e:\n",
    "            print(f\"Error predicting {adv_p}: {e}\")\n",
    "            pred_class_name = \"Error\"\n",
    "        \n",
    "        ax0, ax1, ax2 = axs[i] \n",
    "        \n",
    "        # Original \n",
    "        ax0.imshow(orig_img) \n",
    "        ax0.set_title(f\"Original (normalized)\\nClass: {true_class_name}\", fontsize=9) \n",
    "        ax0.axis(\"off\") \n",
    "        \n",
    "        # Adversarial \n",
    "        ax1.imshow(adv_img_display) \n",
    "        ax1.set_title(f\"Adversarial (normalized)\\nClass: {true_class_name}\", fontsize=9) \n",
    "        ax1.axis(\"off\") \n",
    "        \n",
    "        # Difference heatmap \n",
    "        diff = np.mean(np.abs(adv_img_display - orig_img), axis=2) \n",
    "        p99 = np.percentile(diff, 90) \n",
    "        diff_clipped = np.clip(diff / (p99 + 1e-12), 0, 10) \n",
    "        diff_vis = diff_clipped ** 0.5 \n",
    "        im = ax2.imshow(diff_vis, cmap='hot_r', interpolation='nearest') \n",
    "        \n",
    "        ax2.set_title(f\"Diff heatmap\\nTrue: {true_class_name}, Pred: {pred_class_name}\", fontsize=9) \n",
    "        ax2.axis(\"off\") \n",
    "        ax2.text( \n",
    "                 0.9, 0.1, \n",
    "                 f\"Normalize difference: {diff.mean():.4f}\", \n",
    "                 color='white', \n",
    "                 fontsize=9, \n",
    "                 ha='right', \n",
    "                 va='bottom', \n",
    "                 transform=ax2.transAxes, \n",
    "                 bbox=dict(facecolor='black', alpha=0.8, pad=2) \n",
    "                ) \n",
    "        cbar = fig.colorbar(im, ax=ax2, fraction=0.046, pad=0.02)\n",
    "        cbar.ax.tick_params(labelsize=8)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
